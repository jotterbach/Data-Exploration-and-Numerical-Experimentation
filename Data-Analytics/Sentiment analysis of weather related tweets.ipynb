{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sentiment analysis of weather related tweets\n",
    "\n",
    "This notebook explores some initial modeling of sentiment analysis of weather related tweets. The datasets can be downloaded from Kaggle's \"[Partly Sunny with a Chance of Hashtags](https://www.kaggle.com/c/crowdflower-weather-twitter/data)\" competition.\n",
    "\n",
    "Since the competition is over we will restrict ourselves to look at the training set only in order to have a good basis for benchmarking. We will make heavy use of *nltk* as a natural language processing (NLP) library to build a *bag-of-words* model that we we will use to train a Multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Retrieving and basic cleansing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.encoding = 'ascii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### import the different corpora and packages from nltk as you need them\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>state</th>\n",
       "      <th>location</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>...</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k9</th>\n",
       "      <th>k10</th>\n",
       "      <th>k11</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>k15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jazz for a Rainy Afternoon:  {link}</td>\n",
       "      <td>oklahoma</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT: @mention: I love rainy days.</td>\n",
       "      <td>florida</td>\n",
       "      <td>Miami-Ft. Lauderdale</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good Morning Chicago! Time to kick the Windy C...</td>\n",
       "      <td>idaho</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Preach lol! :) RT @mention: #alliwantis this t...</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>Minneapolis-St. Paul</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@mention good morning sunshine</td>\n",
       "      <td>rhode island</td>\n",
       "      <td>Purgatory</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.597</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet         state  \\\n",
       "id                                                                    \n",
       "1                 Jazz for a Rainy Afternoon:  {link}      oklahoma   \n",
       "2                    RT: @mention: I love rainy days.       florida   \n",
       "3   Good Morning Chicago! Time to kick the Windy C...         idaho   \n",
       "6   Preach lol! :) RT @mention: #alliwantis this t...     minnesota   \n",
       "9                      @mention good morning sunshine  rhode island   \n",
       "\n",
       "                location  s1  s2  s3     s4     s5     w1  w2 ...   k6     k7  \\\n",
       "id                                                            ...               \n",
       "1               Oklahoma   0   0   1  0.000  0.000  0.800   0 ...    0  0.000   \n",
       "2   Miami-Ft. Lauderdale   0   0   0  1.000  0.000  0.196   0 ...    0  0.000   \n",
       "3                    NaN   0   0   0  0.000  1.000  0.000   0 ...    0  1.000   \n",
       "6   Minneapolis-St. Paul   0   0   0  1.000  0.000  1.000   0 ...    0  0.604   \n",
       "9              Purgatory   0   0   0  0.403  0.597  1.000   0 ...    0  0.000   \n",
       "\n",
       "    k8     k9  k10  k11  k12    k13  k14  k15  \n",
       "id                                             \n",
       "1    0  0.000    1    0    0  0.000    0    0  \n",
       "2    0  0.000    1    0    0  0.000    0    0  \n",
       "3    0  0.000    0    0    0  0.000    0    0  \n",
       "6    0  0.196    0    0    0  0.201    0    0  \n",
       "9    0  0.000    0    0    0  1.000    0    0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.DataFrame.from_csv('./datasets/train.csv')\n",
    "raw_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness we will also show a snippet from the test set to see what kind of features we are allowed to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>state</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edinburgh peeps is it sunny?? #weather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birmingham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SEEVERE T’STORM WARNING FOR TROUSDALE,  NORTHW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nashville</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet state    location\n",
       "id                                                                     \n",
       "4              Edinburgh peeps is it sunny?? #weather   NaN  birmingham\n",
       "5   SEEVERE T?STORM WARNING FOR TROUSDALE,  NORTHW...   NaN   Nashville"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame.from_csv('./datasets/test.csv')\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Label creation for Sentiment category\n",
    "\n",
    "We start out by trying to model the sentiment category of a tweet. To this end we start by replacing the maximum vlaue of the sentiment fields s1-s5 with the corresponding category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_label(row):\n",
    "    # row is of time pandas.Series need to cast to a list.\n",
    "    lst = row.tolist()\n",
    "    return lst.index(max(lst))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1    3\n",
       "2    4\n",
       "3    5\n",
       "6    4\n",
       "9    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply defaults to work on columns rather than rows. Use axis = 1 to cahnge to row application.\n",
    "label_df = raw_df[['s1','s2','s3','s4','s5']].apply(create_label, axis=1)\n",
    "label_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tokenizing, stemming and term counting\n",
    "\n",
    "In order to use the tweets as input for a machine learning algorithm we need to convert them into numerical features. One way to do this is to chop up the tweets and count the number of words within a tweet and turn them into a large sparse vector whose length is size of the vocabulary of all tweets.\n",
    "\n",
    "We start with a very simple way of counting the terms in the total of all tweets and count the number of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 36622),\n",
       " ('weather', 23046),\n",
       " ('to', 20942),\n",
       " ('@mention', 19424),\n",
       " ('a', 18712),\n",
       " ('in', 18611),\n",
       " ('and', 17827),\n",
       " ('i', 16907),\n",
       " ('is', 14302),\n",
       " ('{link}', 13491),\n",
       " ('for', 12580),\n",
       " ('this', 12231),\n",
       " ('of', 10703),\n",
       " ('it', 9131),\n",
       " ('rt', 8999),\n",
       " ('on', 8060),\n",
       " ('@mention:', 8008),\n",
       " ('my', 7492),\n",
       " (\"it's\", 7130),\n",
       " ('at', 6974),\n",
       " ('out', 6397),\n",
       " ('be', 6122),\n",
       " ('its', 5900),\n",
       " ('storm', 5673),\n",
       " ('you', 5385)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "full_tweet_string = raw_df.tweet.apply(lambda t: t.lower() + \" \").sum()\n",
    "Counter(full_tweet_string.split()).most_common()[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple counter already indicates a problem. There are a lot of very common words that have obiously no signal, such as 'the', 'at', 'of', etc. We need to remove those stopwords. Another problem can be seem by comparing '@mention' and '@mention:' which should clearly be identified as the same word, meaning we need to remove the punctuation. Finally we might want to identify 'storm' and 'stormy' as the same and thus require stemming techniques.\n",
    "\n",
    "The following code snippet provides a tokenizer that does all of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "\n",
    "def custom_tokenizer(document_string):\n",
    "    \n",
    "    # define the stop word vocabulary\n",
    "    stops = [unicode(word) for word in stopwords.words('english')] \\\n",
    "        + [\"''\", \"``\", 're:', 'fwd:', '-', '@mention', '@mention:', 'mention', 'link', ':', 'f.', '&'] \n",
    "    \n",
    "    # create a default stemmer\n",
    "    stemmer = EnglishStemmer()\n",
    "    \n",
    "    # return the stemmed list of words\n",
    "    tokens = [stemmer.stem(unicode(word)) for word in word_tokenize(document_string.lower()) \\\n",
    "            if not (unicode(word.lower()) in stops or unicode(word.lower()) in list(string.punctuation))]\n",
    "    \n",
    "    return tokens + list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the word counts again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'weather', 34001),\n",
       " (u'...', 17465),\n",
       " (u\"'s\", 12792),\n",
       " (u'rt', 9189),\n",
       " (u'day', 8824),\n",
       " (u'storm', 7920),\n",
       " (u'sunni', 6601),\n",
       " (u'hot', 5952),\n",
       " (u'today', 5612),\n",
       " (u'outsid', 5357),\n",
       " (u'like', 4831),\n",
       " (u\"n't\", 4714),\n",
       " (u'rain', 4655),\n",
       " (u'sunshin', 4587),\n",
       " (u'get', 4550),\n",
       " (u'degre', 4441),\n",
       " (u'thunderstorm', 4388),\n",
       " (u'feel', 4339),\n",
       " (u'humid', 4229),\n",
       " (u'go', 4164),\n",
       " (u'cold', 4041),\n",
       " (u\"'m\", 4032),\n",
       " (u'wind', 3913),\n",
       " (u'raini', 3889),\n",
       " (u'good', 3766)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(custom_tokenizer(full_tweet_string)).most_common()[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already much better and we can identify weather realated terms such as 'storm', 'sunni', etc.\n",
    "This will be a good starting point for the rest of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Model generation\n",
    "\n",
    "One step that is missing is how to use the *custom_tokenizer* to actually create feature vectors. Luckily enough sklearn provides use with the right functionality to do just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 100, max_df = 10000, tokenizer = custom_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we have always the same vectorization options we need to \"fit\" the vectorizer to the set so we can vectorize any new examples according to this. We do it here on the full set from which we will also extract the cross-validation data and we should be aware of this bias, as it necessarily eliminates the possibility of encountering previously unseen terms when we run our test set predicition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the vectorizer to the full set\n",
    "X = vectorizer.fit_transform(raw_df.tweet.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77946, 1260)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Splitting the set and training the model\n",
    "\n",
    "As the training data and the labels are now spearate and of different type it is annoying to split the training and test-sets manually. Again sklearn has a tool for us that we can use: the cross_validation API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "df_train, df_test, y_train, y_test = cross_validation.train_test_split(raw_df, label_df, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the above case of having the \"perfect\" vocabulary let us now train a new vectorizer using only the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(pd.DataFrame(df_train)[0].tolist())\n",
    "X_test = vectorizer.transform(pd.DataFrame(df_test)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to train the model. As this is a highly sparse problem it lends itself to be tackled using a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60549948682860077"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multi_nb_clf = MultinomialNB()\n",
    "multi_nb_clf.fit(X_train, y_train)\n",
    "\n",
    "multi_nb_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only single terms (monograms), bigrams (interestingly enough they seem not to have any impact at this level) and no additionally fine-grained modeling we already classify 60% of the tweets into the right category. This is impressive in so far that there are a lot of easy and obvious ways to improve the classification. Some ways to improve are\n",
    "\n",
    "* include higher n-grams in the tokenizer\n",
    "* train models per state or per city\n",
    "* include state and city as features\n",
    "* include TF-IDF vectorizers\n",
    "* use a different classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Miscellenea\n",
    "###References\n",
    "\n",
    "are mostly given throughout the text. But importantly\n",
    "\n",
    "- [1] [Partly Sunny with a Chance of Hashtags](https://www.kaggle.com/c/crowdflower-weather-twitter)\n",
    "- [2] [scikit-learn](http://scikit-learn.org/stable/index.html)\n",
    "- [3] [nltk](http://www.nltk.org/index.html)\n",
    "\n",
    "###Stylesheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
